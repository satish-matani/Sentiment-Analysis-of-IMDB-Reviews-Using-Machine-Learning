{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8baff0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I thought this was a wonderful way to spend ti...  positive\n",
       "1  Probably my all-time favorite movie, a story o...  positive\n",
       "2  I sure would like to see a resurrection of a u...  positive\n",
       "3  This show was an amazing, fresh & innovative i...  negative\n",
       "4  Encouraged by the positive comments about this...  negative"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the IMDB dataset\n",
    "file_path = 'IMDB.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Displaying first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2bd7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885a39f",
   "metadata": {},
   "source": [
    "## Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and Lemmatize/Stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b832150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/satishmatani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/satishmatani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/satishmatani/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>probably alltime favorite movie story selfless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>show amazing fresh innovative idea 70 first ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>encouraged positive comment film looking forwa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  I thought this was a wonderful way to spend ti...   \n",
       "1  Probably my all-time favorite movie, a story o...   \n",
       "2  I sure would like to see a resurrection of a u...   \n",
       "3  This show was an amazing, fresh & innovative i...   \n",
       "4  Encouraged by the positive comments about this...   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  thought wonderful way spend time hot summer we...  \n",
       "1  probably alltime favorite movie story selfless...  \n",
       "2  sure would like see resurrection dated seahunt...  \n",
       "3  show amazing fresh innovative idea 70 first ai...  \n",
       "4  encouraged positive comment film looking forwa...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources for stopwords and lemmatization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text Preprocessing Functions\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the review column\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "df[['review', 'cleaned_review']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369546d1",
   "metadata": {},
   "source": [
    "This step ensured that the model would now focus on the meaningful content of the reviews without distractions from common words or formatting issues as we've removed them from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e32b56",
   "metadata": {},
   "source": [
    "## Perform TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630f545f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 5000),\n",
       " array(['10', '100', '1000', ..., 'zero', 'zombie', 'zone'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Apply TFIDF Vectorization on the cleaned reviews\n",
    "X_tfidf = tfidf_vect.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Display the shape of the resulting TFIDF matrix\n",
    "X_tfidf.shape, tfidf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519dff3",
   "metadata": {},
   "source": [
    "This method highlighted important words in the reviews based on their frequency across the dataset, which will help the models to capture the sentiment more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f00b8",
   "metadata": {},
   "source": [
    "## Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting/Xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ca1235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Random Forest Best Params: {'max_depth': None, 'n_estimators': 100}\n",
      "Gradient Boosting Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Encode the sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grids for both Random Forest and Gradient Boosting\n",
    "rf_params = {'n_estimators': [50, 100], 'max_depth': [10, 20, None]}\n",
    "gb_params = {'n_estimators': [50, 100], 'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.05]}\n",
    "\n",
    "# Initialize Random Forest and Gradient Boosting models\n",
    "rf_model = RandomForestClassifier()\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Initialize GridSearchCV for both models\n",
    "rf_grid = GridSearchCV(rf_model, rf_params, cv=3, n_jobs=-1, verbose=1)\n",
    "gb_grid = GridSearchCV(gb_model, gb_params, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit both models using GridSearchCV\n",
    "rf_grid.fit(X_train, y_train)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters for both models\n",
    "print(\"Random Forest Best Params:\", rf_grid.best_params_)\n",
    "print(\"Gradient Boosting Best Params:\", gb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2b5a2",
   "metadata": {},
   "source": [
    "Both models were trained on the training dataset after splitting it from the original data. Hyperparameters were optimized using GridSearchCV, which allowed me to systematically evaluate different combinations of parameters to find the best settings for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8f601",
   "metadata": {},
   "source": [
    "## Perform Final evaluation of models on the best parameter settings using the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b50e8641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Confusion Matrix: \n",
      "\n",
      " [[2135  392]\n",
      " [ 379 2094]]\n",
      "\n",
      "Random Forest Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      2527\n",
      "           1       0.84      0.85      0.84      2473\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n",
      "Gradient Boosting Confusion Matrix: \n",
      "\n",
      " [[2010  517]\n",
      " [ 330 2143]]\n",
      "\n",
      "Gradient Boosting Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83      2527\n",
      "           1       0.81      0.87      0.83      2473\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.83      0.83      0.83      5000\n",
      "weighted avg       0.83      0.83      0.83      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Final evaluation of the models\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_best = rf_grid.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "gb_best = gb_grid.best_estimator_\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "\n",
    "# Calculate and display evaluation metrics\n",
    "print(\"Random Forest Confusion Matrix: \\n\\n\",confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nRandom Forest Classification Report: \\n\\n\",classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Gradient Boosting Confusion Matrix: \\n\\n\",confusion_matrix(y_test, y_pred_gb))\n",
    "print(\"\\nGradient Boosting Classification Report: \\n\\n\",classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ad9d9",
   "metadata": {},
   "source": [
    "## Include a Conclusion section, compare the model performance, report the best performing model, and include key insights obtained from the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25f1aa",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "In this project, I analyzed IMDB movie reviews to predict sentiment using Random Forest and Gradient Boosting models using GridSearchCV. After preprocessing the data and applying TFIDF vectorization, I compared the performance of both models, whose details are as below:\n",
    "\n",
    "## Comparing Model Performance:\n",
    "\n",
    "Random Forest achieved an accuracy of 85% with a balanced precision and recall for both classes (0.85 and 0.84).\n",
    "\n",
    "Gradient Boosting had an accuracy of 83%, with slightly lower precision (0.86 for class 0, 0.81 for class 1) and recall (0.80 for class 0, 0.87 for class 1).\n",
    "\n",
    "## Best performing model:\n",
    "\n",
    "The Random Forest model outperformed Gradient Boosting, making it the better choice for this sentiment analysis task.\n",
    "\n",
    "## Key insights:\n",
    "\n",
    "Both models effectively classified movie sentiments, but Random Forest was more accurate.\n",
    "\n",
    "The confusion matrices revealed some misclassifications, with Random Forest misclassifying 392 positive reviews as negative, while Gradient Boosting misclassified 517 negative reviews as positive.\n",
    "\n",
    "Overall, this project shows that machine learning techniques like Random Forest can be effective for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
